comment: Testing zero-sum simple reward structure with low intended action execution probabilities to see if offline solver can find and optimal strategy to beat the heuristic agent when the 
         push action is enabled.


experiment_settings:
  name: "TMDP_DPAgent_PerfectModel_vs_ManhattanAgent_Passive"
  num_runs: 1     # How many times to repeat the experiment with different seeds
  num_episodes: 10000 # Number of episodes per run
  run_seed: 42 # Intial seed for runs
  results_dir: "results/test" # Directory to save plots and data

environment_settings:
  class: "CoinGame"
  params:
    grid_size: 5
    max_steps: 20000
    enable_push: True
    push_distance: 2
    action_execution_probabilities: [0.6, 0.6]
    rewards:
      # --- General Penalties ---
      step_penalty: [-0.1, -0.1]                       # Penalty incurred by each agent on every step.
      out_of_bounds_penalty_delta: [0.0, 0.0]          # Penalty for attempting to move off the grid.
      collision_penalty_delta: [0.0, 0.0]              # Penalty for attempting to move to the same cell as the other player.

      # --- Push-related Rewards/Penalties ---
      push_reward_delta: [0.0, 0.0]                       # Reward for successfully pushing an adjacent opponent.
      push_penalty_delta: [0.0, 0.0]                      # Penalty for being pushed by an opponent.
      push_but_not_adjacent_penalty_delta: [0.0, 0.0]     # Penalty for using the 'push' action when not next to the opponent.
      both_push_penalty_delta: [0.0, 0.0]                 # Penalty applied to both players if they push each other simultaneously.

      # --- Coin-related Rewards/Penalties ---
      coin_reward_delta: [0.0, 0.0]                       # Reward for collecting a coin.
      coin_steal_penalty_delta: [0.0, 0.0]                # Penalty for the other player when one player collects a coin.
      contested_coin_penalty_delta: [0.0, 0.0]            # Penalty applied to both players if they try to collect same coin at the same time.

      # --- Terminal (End of Game) Rewards/Penalties ---
      win_reward: [10.0, 10.0]                      # Reward for winning the game (collecting both coins).
      loss_penalty: [-10.0, -10.0]                  # Penalty for losing the game.
      draw_penalty: [0.0, 0.0]                      # Penalty for a draw (both coins collected, but neither player won).

      # --- Timeout-related Rewards/Penalties (if max_steps is reached) ---
      timeout_penalty_delta: [-2.0, -2.0]                # Base penalty applied to both players if the game times out.
      timeout_lead_bonus_delta: [1.0, 1.0]               # Additional bonus for the player with more coins at timeout.
      timeout_trail_penalty_delta: [-1.0, -1.0]          # Additional penalty for the player with fewer coins at timeout.


agent_settings:
  player_1:
    class: TMDP_DP_Agent_PerfectModel
    params:
      gamma: 0.95
      initial_V_value: 0

      termination_criterion: 0.00001        # Convergence threshold for value iteration.
      value_iteration_max_num_of_iter: 1000 # Max iterations for value iteration.


  player_2:
    class: ManhattanAgent_Passive


plotting_settings:
  plot_reward_bands: True
  plot_moving_average_window_size: 500
  reward_time_series_x_axis_plot_range: [0,9999]

  plot_result_ratio_bands: True
  episode_range_to_eval: [9899,9999]
  game_result_ratio_x_axis_plot_range: [9899,9999]
    