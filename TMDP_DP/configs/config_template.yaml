# ---------------------------------------------------------------------------
# EXPERIMENT SETTINGS
# Defines the overall structure of the experimental run.
# ---------------------------------------------------------------------------
comment: User defined comment sepecifing details of the run. This yaml entry is not used in the code.

experiment_settings:
  # A descriptive name for the experiment. This will be used in the output folder name.
  name: "Experiment_Name_Here"

  # The number of times to repeat the entire experiment with different random seeds.
  # This is crucial for ensuring that results are statistically significant.
  num_runs: 5

  # The number of episodes (games) to run within each experimental run.
  num_episodes: 10000

  # Starting seed of independent experiment runs
  # NOTE: Each succesive run has seed run_seed + 1
  # Leaving this value empty will run the experiments with random seed
  run_seed: 42

  # The base directory where all results (plots, logs, config copies) will be saved.
  # A timestamped sub-folder will be created inside this directory for each run.
  results_dir: "results/"

# ---------------------------------------------------------------------------
# ENVIRONMENT SETTINGS
# Configures the game world in which the agents will interact.
# ---------------------------------------------------------------------------
environment_settings:
  # The class name of the environment to use, as defined in `engine_DP.py`.
  class: "CoinGame"
  params:
    # The size of the square grid (N x N).
    grid_size: 5

    # The maximum number of steps allowed in a single episode before it times out.
    max_steps: 20000

    # The number of grid cells an opponent is moved when successfully pushed.
    push_distance: 2

    # This dictionary controls the rewards and penalties within the CoinGame environment.
    # NOTE: _delta suffix indicates the value is added to the running reward total starting from step_penalty.
    #       Values without the suffix will overwrite the running total.
    rewards:
      # --- General Penalties ---
      step_penalty: -0.1                   # Penalty incurred by each agent on every step.
      out_of_bounds_penalty_delta: -0.5          # Penalty for attempting to move off the grid.
      collision_penalty_delta: -0.05             # Penalty for attempting to move to the same cell as the other player.

      # --- Push-related Rewards/Penalties ---
      push_reward_delta: 0.4                     # Reward for successfully pushing an adjacent opponent.
      push_penalty_delta: -0.2                   # Penalty for being pushed by an opponent.
      push_but_not_adjacent_penalty_delta: -0.05 # Penalty for using the 'push' action when not next to the opponent.
      both_push_penalty_delta: -0.05             # Penalty applied to both players if they push each other simultaneously.

      # --- Coin-related Rewards/Penalties ---
      coin_reward_delta: 2.0                     # Reward for collecting a coin.
      coin_steal_penalty_delta: -0.5             # Penalty for the other player when one player collects a coin.
      contested_coin_penalty_delta: -0.2         # Penalty applied to both players if they try to collect same coin at the same time.

      # --- Terminal (End of Game) Rewards/Penalties ---
      win_reward: 10.0                     # Reward for winning the game (collecting both coins).
      loss_penalty: -10.0                  # Penalty for losing the game.
      draw_penalty: -5.0                   # Penalty for a draw (both coins collected, but neither player won).

      # --- Timeout-related Rewards/Penalties (if max_steps is reached) ---
      timeout_penalty_delta: -2.0                # Base penalty applied to both players if the game times out.
      timeout_lead_bonus_delta: 1.0              # Additional bonus for the player with more coins at timeout.
      timeout_trail_penalty_delta: -1.0          # Additional penalty for the player with fewer coins at timeout.

# ---------------------------------------------------------------------------
# AGENT SETTINGS
# Defining the agents Player 1 and Player 2.
# You can mix and match any agent types from `agents` module.
# ---------------------------------------------------------------------------
agent_settings:
  player_1:
    # The class name of the agent for Player 1 (must exist in the `agents` module).
    # --- AVAILABLE AGENT CLASSES ---
    # Heuristic Agents:       ManhattanAgent, ManhattanAgent_Passive, ManhattanAgent_Aggressive, ManhattanAgent_Ultra_Aggressive
    # Q-Learning Agents:      IndQLearningAgent, IndQLearningAgentSoftmax, LevelKQAgent, LevelKQAgentSoftmax
    # MDP DP Agents:          LevelK_MDP_DP_Agent_Stationary, LevelK_MDP_DP_Agent_NonStationary, LevelK_MDP_DP_Agent_Dynamic
    # TMDP DP Agents:         LevelK_TMDP_DP_Agent_Stationary, LevelK_TMDP_DP_Agent_NonStationary, LevelK_TMDP_DP_Agent_Dynamic
    # Offline Solvers:        DPAgent_PerfectModel, TMDP_DPAgent_PerfectModel
    class: "LevelKQAgent"

    # Agent-specific parameters that are passed to its constructor.
    # The `experiment_runner.py` will validate that all required parameters are provided.
    params:
      # --- Common Parameters (for most learning agents) ---
      gamma: 0.95              # Discount factor for future rewards.
      learning_rate: 0.5       # The learning rate (alpha) for Q-learning agents.
      epsilon: 1.0             # The initial exploration rate for epsilon-greedy policies.

      # --- Parameters for Level-K Agents (LevelKQ, LevelK_MDP_DP, LevelK_TMDP_DP) ---
      k: 2                     # The cognitive level of the agent.
      lower_level_k_epsilon: 0.1 # The epsilon for the agent's *internal* models of the opponent (k-1 and lower).
      
      # --- Parameters for Q-Learning Agents (LevelKQ, IndQLearning) ---
      initial_Q_value: 0.0     # Initial value for Q-tables.
      
      # --- Parameters for DP Agents (LevelK_MDP_DP, LevelK_TMDP_DP, offline solvers) ---
      initial_V_value: 0.0     # Initial value for V-tables.

      # --- Parameters for Softmax Agents (IndQLearningAgentSoftmax, LevelKQAgentSoftmax) ---
      beta: 1.0                # The temperature parameter for the softmax policy. Higher values lead to more deterministic actions.

      # --- Parameters for Offline Solvers (DPAgent_PerfectModel, TMDP_DPAgent_PerfectModel) ---
      # NOTE: The 'opponent' param is configured as player_2 and handled automatically by the runner.
      termination_criterion: 0.001        # Convergence threshold for value iteration.
      value_iteration_max_num_of_iter: 1000 # Max iterations for value iteration.

    # Defines the epsilon decay schedule for the main agent (Player 1).
    epsilon_decay_agent:
      type: "linear"           # Options: "linear", "no_decay"
      end: 0.1                 # The final epsilon value.

    # Defines the epsilon decay for the k-1 cognitive hierarchy.
    # This is only used for k-level agents.
    epsilon_decay_inernal_opponent_model:
      type: "linear"           # Options: "linear", "no_decay"
      end: 0.1                 # The final epsilon value.

  player_2:
    class: "IndQLearningAgentSoftmax"
    params:
      # Common params
      gamma: 0.95
      learning_rate: 0.5
      epsilon: 1.0

      # Softmax specific
      beta: 1.0 # NOTE: Epsilon is not used for action selection by Softmax agents.

      # Level-K specific (not used by IndQLearningAgentSoftmax, but shown for completeness)
      k: 1
      initial_Q_value: 0.0
      lower_level_k_epsilon: 0.1


    # Epsilon decay for the agent itself.
    # NOTE: IndQLearningAgentSoftmax uses a softmax policy, so `epsilon` and its decay are only used
    # if you were to modify its `act` method to be epsilon-greedy. For the current implementation,
    # this section would be ignored for this specific agent. It is kept for consistency.

    # NOTE: Softmax agents do not use an epsilon-greedy policy for action selection,
    # so this section is ignored by IndQLearningAgentSoftmax
    epsilon_decay_agent:
      type: "linear"
      end: 0.1

    # Epsilon decay for the internal opponent model (not applicable to IQL agents).
    epsilon_decay_inernal_opponent_model:
      type: "linear"
      end: 0.1

# ---------------------------------------------------------------------------
# PLOTTING SETTINGS
# Configures the appearance of the final reward plot.
# ---------------------------------------------------------------------------
plotting_settings:
  # Range of values to plot in the reward time series.
  reward_time_series_range: [9000,10000]
  # Range of values to plot in the win, loss and draw ratio time series.
  game_result_ratio_range: [9000,10000]
  # The size of the window for the moving average applied to the reward curves.
  # This helps to smooth out the noise and see the underlying learning trend.
  moving_average_window: 100