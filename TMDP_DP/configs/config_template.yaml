# ---------------------------------------------------------------------------
# EXPERIMENT SETTINGS
# Defines the overall structure of the experimental run.
# ---------------------------------------------------------------------------
comment: User defined comment sepecifing details of the run. This yaml entry is not used in the code.

experiment_settings:
  # A descriptive name for the experiment. This will be used in the output folder name.
  name: "Experiment_Name_Here"

  # The number of times to repeat the entire experiment with different random seeds.
  # This is crucial for ensuring that results are statistically significant.
  num_runs: 5

  # The number of episodes (games) to run within each experimental run.
  num_episodes: 10000

  # Starting seed of independent experiment runs
  # NOTE: Each succesive run has seed run_seed + 1
  # Leaving this value empty will run the experiments with random seed
  run_seed: 42

  # The base directory where all results (plots, logs, config copies) will be saved.
  # A timestamped sub-folder will be created inside this directory for each run.
  results_dir: "results/"

# ---------------------------------------------------------------------------
# ENVIRONMENT SETTINGS
# Configures the game world in which the agents will interact.
# ---------------------------------------------------------------------------
environment_settings:
  # The class name of the environment to use, as defined in `engine_DP.py`.
  class: "CoinGame"
  params:
    # The size of the square grid (N x N).
    grid_size: 5

    # The maximum number of steps allowed in a single episode before it times out.
    max_steps: 20000

    # If set to `false`, the "push" action is disabled for the entire experiment.
    # Agents will only have access to move actions, and all push-related rewards/penalties will be ignored.
    enable_push: True

    # The number of grid cells an opponent is moved when successfully pushed.
    push_distance: 2

    # List of probability of intended action being executed, these are the probabilities governing the transition distribution of  
    # the environement
    # NOTE: First element corresponds to player 0 and second element corresponds to player 1.
    action_execution_probabilities: [0.8, 0.8]

    # This dictionary controls the rewards and penalties within the CoinGame environment.
    # NOTE: _delta suffix indicates the value is added to the running reward total starting from step_penalty.
    #       Values without the suffix will overwrite the running total.
    # NOTE: First element corresponds to player 0 and second element corresponds to player 1.
    rewards:
      # --- General Penalties ---
      step_penalty: [-0.1, -0.1]                       # Penalty incurred by each agent on every step.
      out_of_bounds_penalty_delta: [0.0, 0.0]          # Penalty for attempting to move off the grid.
      collision_penalty_delta: [0.0, 0.0]              # Penalty for attempting to move to the same cell as the other player.

      # --- Push-related Rewards/Penalties ---
      push_reward_delta: [0.4, 0.4]                       # Reward for successfully pushing an adjacent opponent.
      push_penalty_delta: [-0.2, -0.2]                    # Penalty for being pushed by an opponent.
      push_but_not_adjacent_penalty_delta: [-0.05, -0.05] # Penalty for using the 'push' action when not next to the opponent.
      both_push_penalty_delta: [-0.05, -0.05]             # Penalty applied to both players if they push each other simultaneously.

      # --- Coin-related Rewards/Penalties ---
      coin_reward_delta: [0.0, 0.0]                       # Reward for collecting a coin.
      coin_steal_penalty_delta: [0.0, 0.0]                # Penalty for the other player when one player collects a coin.
      contested_coin_penalty_delta: [0.0, 0.0]            # Penalty applied to both players if they try to collect same coin at the same time.

      # --- Terminal (End of Game) Rewards/Penalties ---
      win_reward: [10.0, 10.0]                      # Reward for winning the game (collecting both coins).
      loss_penalty: [-10.0, -10.0]                  # Penalty for losing the game.
      draw_penalty: [0.0, 0.0]                      # Penalty for a draw (both coins collected, but neither player won).

      # --- Timeout-related Rewards/Penalties (if max_steps is reached) ---
      timeout_penalty_delta: [-2.0, -2.0]                # Base penalty applied to both players if the game times out.
      timeout_lead_bonus_delta: [1.0, 1.0]               # Additional bonus for the player with more coins at timeout.
      timeout_trail_penalty_delta: [-1.0, -1.0]          # Additional penalty for the player with fewer coins at timeout.

# ---------------------------------------------------------------------------
# AGENT SETTINGS
# Defining the agents Player 1 and Player 2.
# You can mix and match any agent types from `agents` module.
# ---------------------------------------------------------------------------
agent_settings:
  player_1:
    # The class name of the agent for Player 1 (must exist in the `agents` module).
    # --- AVAILABLE AGENT CLASSES ---
    # Heuristic Agents:       ManhattanAgent, ManhattanAgent_Passive, ManhattanAgent_Aggressive, ManhattanAgent_Ultra_Aggressive
    # Q-Learning Agents:      IndQLearningAgent, IndQLearningAgentSoftmax, LevelKQAgent, LevelKQAgentSoftmax
    # MDP DP Agents:          LevelK_MDP_DP_Agent_Stationary, LevelK_MDP_DP_Agent_NonStationary, LevelK_MDP_DP_Agent_Dynamic
    # TMDP DP Agents:         LevelK_TMDP_DP_Agent_Stationary, LevelK_TMDP_DP_Agent_NonStationary, LevelK_TMDP_DP_Agent_Dynamic
    # Offline Solvers:        MDP_DP_Agent_PerfectModel, TMDP_DP_Agent_PerfectModel
    class: "LevelKQAgent"

    # Agent-specific parameters that are passed to its constructor.
    # The `experiment_runner.py` will validate that all required parameters are provided.
    params:
      # --- Common Parameters (for most learning agents) ---
      gamma: 0.95              # Discount factor for future rewards.
      learning_rate: 0.5       # The learning rate (alpha) for Q-learning agents.
      epsilon: 1.0             # The initial exploration rate for epsilon-greedy policies.

      # --- Parameters for Level-K Agents (LevelKQ, LevelK_MDP_DP, LevelK_TMDP_DP) ---
      k: 2                     # The cognitive level of the agent.
      lower_level_k_epsilon: 0.1 # The epsilon for the agent's *internal* models of the opponent (k-1 and lower).
      
      # --- Parameters for Q-Learning Agents (LevelKQ, IndQLearning) ---
      initial_Q_value: 0.0     # Initial value for Q-tables.
      
      # --- Parameters for DP Agents (LevelK_MDP_DP, LevelK_TMDP_DP, offline solvers) ---
      initial_V_value: 0.0     # Initial value for V-tables.

      # --- Parameters for Softmax Agents (IndQLearningAgentSoftmax, LevelKQAgentSoftmax) ---
      beta: 1.0                # The temperature parameter for the softmax policy. Higher values lead to more deterministic actions.

      # --- Parameters for Offline Solvers (MDP_DP_Agent_PerfectModel, TMDP_DP_Agent_PerfectModel) ---
      # NOTE: The 'opponent' param is configured as player_2 and handled automatically by the runner.
      termination_criterion: 0.001        # Convergence threshold for value iteration.
      value_iteration_max_num_of_iter: 1000 # Max iterations for value iteration.

    # Defines the epsilon decay schedule for the main agent.
    epsilon_decay_agent:
      type: "linear"           # Options: "linear", "no_decay"
      end: 0.1                 # The final epsilon value for "linear" option.

    # Defines the epsilon decay for the k-1 cognitive hierarchy.
    # This is only used for k-level agents.
    epsilon_decay_internal_opponent_model:
      type: "linear"           # Options: "linear", "no_decay"
      end: 0.1                 # The final epsilon value for "linear" option.

  player_2:
    class: "IndQLearningAgentSoftmax"
    params:
      # Common params
      gamma: 0.95
      learning_rate: 0.5
      epsilon: 1.0

      # Softmax specific
      beta: 1.0 # NOTE: Epsilon is not used for action selection by Softmax agents.

      # Level-K specific (not used by IndQLearningAgentSoftmax, but shown for completeness)
      k: 1
      initial_Q_value: 0.0
      lower_level_k_epsilon: 0.1


    # Epsilon decay for the agent itself.
    # NOTE: IndQLearningAgentSoftmax uses a softmax policy, so `epsilon` and its decay are only used
    # if you were to modify its `act` method to be epsilon-greedy. For the current implementation,
    # this section would be ignored for this specific agent. It is kept for consistency.

    # NOTE: Softmax agents do not use an epsilon-greedy policy for action selection,
    # so this section is ignored by IndQLearningAgentSoftmax
    epsilon_decay_agent:
      type: "linear"
      end: 0.1

    # Epsilon decay for the internal opponent model (not applicable to IQL agents).
    epsilon_decay_internal_opponent_model:
      type: "linear"
      end: 0.1

# ---------------------------------------------------------------------------
# PLOTTING SETTINGS
# Configures the appearance of the final reward plot.
# ---------------------------------------------------------------------------
plotting_settings:

  # If True, plots a shaded 95% confidence interval band around the mean reward.
  # If False, plots the individual reward curve for each experiment run with low opacity.
  # The band is recommended for a large number of runs to avoid visual clutter.
  plot_reward_bands: True
  # The size of the window for the moving average applied to the reward curves.
  # This helps to smooth out the noise and see the underlying learning trend.
  moving_average_window: 100
  # Sets the [xmin, xmax] range for the x-axis on the reward plots.
  reward_time_series_x_axis_plot_range: [9000,10000]

  # If True, plots a 95% confidence interval band for the game result ratios.
  # If False, plots the individual ratio curve for each run with low opacity.
  plot_result_ratio_bands: True
  # Defines the episode slice [start, end] to analyze for win/loss/draw ratios.
  episode_range_to_eval: [9899, 9999]
  # Sets the [xmin, xmax] range for the x-axis on the game result ratio plots.
  game_result_ratio_x_axis_plot_range: [9899, 9999]
