{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T16:26:31.229451Z",
     "start_time": "2018-12-27T16:26:31.008474Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Video\n",
    "# Also ffmpeg is required for video saving\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# Importing custom modules\n",
    "from engine_DP import CoinGame\n",
    "from agent_DP import RandomAgent, IndQLearningAgentSoftmax, Level2QAgent, Level2QAgentSoftmax\n",
    "# from agent_DP import ExpSmoother, FPLearningAgent, Level2QAgent, RandomAgent, IndQLearningAgent, Level3QAgent, Level3QAgentMixExp, Level3QAgentMixDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T16:26:31.342933Z",
     "start_time": "2018-12-27T16:26:31.329438Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moving_average(a, n=3):\n",
    "    \"\"\"\n",
    "    Compute the simple moving average of a 1D array.\n",
    "\n",
    "    Parameters:\n",
    "        a (array-like): Input array containing numerical data.\n",
    "        n (int, optional): Window size for the moving average. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Array of moving averages with length len(a) - n + 1.\n",
    "    \"\"\"\n",
    "    # Compute cumulative sum of the input array as float\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    \n",
    "    # Subtract the cumulative sum shifted by 'n' to get the sum over each window\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    \n",
    "    # Divide by window size to obtain the moving average\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "def plot(r0ss, r1ss, moving_average_window_size=1000, dir=None):\n",
    "    \"\"\"\n",
    "    Plot smoothed reward trajectories for two agents over multiple experiments.\n",
    "\n",
    "    Parameters:\n",
    "        r0ss (list of arrays): Rewards for Agent A across experiments.\n",
    "        r1ss (list of arrays): Rewards for Agent B across experiments.\n",
    "        dir (str, optional): If provided, saves the plot to 'dir.png'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Apply 'ggplot' style for aesthetics\n",
    "    plt.style.use('ggplot')\n",
    "\n",
    "    N_EXP = len(r0ss)  # Number of experiments\n",
    "\n",
    "    # Plot individual experiment trajectories with low opacity\n",
    "    for i in range(N_EXP):\n",
    "        plt.plot(moving_average(r0ss[i], moving_average_window_size), 'b', alpha=0.05)\n",
    "        plt.plot(moving_average(r1ss[i], moving_average_window_size), 'r', alpha=0.05)\n",
    "\n",
    "    # Plot average trajectory across experiments with higher opacity\n",
    "    plt.plot(moving_average(np.mean(r0ss, axis=0), moving_average_window_size), 'b', alpha=0.5)\n",
    "    plt.plot(moving_average(np.mean(r1ss, axis=0), moving_average_window_size), 'r', alpha=0.5)\n",
    "\n",
    "    # Label axes\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('R')\n",
    "\n",
    "    # Create custom legend\n",
    "    custom_lines = [Line2D([0], [0], color='b', label='Agent A'),\n",
    "                    Line2D([0], [0], color='r', label='Agent B')]\n",
    "    plt.legend(handles=custom_lines)\n",
    "\n",
    "    # Save plot if directory is specified\n",
    "    if dir is not None:\n",
    "        plt.savefig(f\"{dir}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_trajectory_from_log(trajectory_log, grid_size=4, fps=4, dpi=100):\n",
    "    \"\"\"\n",
    "    Animate a trajectory from an enriched trajectory log that includes actions and rewards.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.set_xlim(-0.5, grid_size - 0.5)\n",
    "    ax.set_ylim(-0.5, grid_size - 0.5)\n",
    "    ax.set_xticks(range(grid_size))\n",
    "    ax.set_yticks(range(grid_size))\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True)\n",
    "\n",
    "    title = ax.set_title(\"\")\n",
    "    \n",
    "    info_text = fig.text(0.02, 0.5, \"\", ha='left', va='center', fontsize=10)\n",
    "    \n",
    "    move_action_codes = [\"Down\", \"Right\", \"Up\", \"Left\"]\n",
    "    action_push_codes = [\"No push\", \"Push\"]\n",
    "\n",
    "    # Agent and coin markers\n",
    "    dm_dot, = ax.plot([], [], 'bo', label='DM (Blue)')\n",
    "    adv_dot, = ax.plot([], [], 'ro', label='Adv (Red)')\n",
    "    coin1_dot, = ax.plot([], [], 'y*', label='Coin 1', markersize=15)\n",
    "    coin2_dot, = ax.plot([], [], 'g*', label='Coin 2', markersize=15)\n",
    "\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "\n",
    "    def init():\n",
    "        dm_dot.set_data([], [])\n",
    "        adv_dot.set_data([], [])\n",
    "        coin1_dot.set_data([], [])\n",
    "        coin2_dot.set_data([], [])\n",
    "        title.set_text(\"\")\n",
    "        return dm_dot, adv_dot, coin1_dot, coin2_dot, title\n",
    "\n",
    "    def update(frame):\n",
    "        state = trajectory_log[frame]\n",
    "\n",
    "        \n",
    "        if isinstance(state['DM_location_new'], (list, np.ndarray)) and len(state['DM_location_new']) == 2:\n",
    "            dm_dot.set_data([state['DM_location_new'][1]], [state['DM_location_new'][0]])\n",
    "        else:\n",
    "            dm_dot.set_data([], [])\n",
    "\n",
    "        if isinstance(state['Adv_location_new'], (list, np.ndarray)) and len(state['Adv_location_new']) == 2:\n",
    "            adv_dot.set_data([state['Adv_location_new'][1]], [state['Adv_location_new'][0]])\n",
    "        else:\n",
    "            adv_dot.set_data([], [])\n",
    "\n",
    "        if isinstance(state['coin1'], (list, np.ndarray)) and len(state['coin1']) == 2:\n",
    "            coin1_dot.set_data([state['coin1'][1]], [state['coin1'][0]])\n",
    "        else:\n",
    "            coin1_dot.set_data([-10], [-10])  # hide\n",
    "\n",
    "        if isinstance(state['coin2'], (list, np.ndarray)) and len(state['coin2']) == 2:\n",
    "            coin2_dot.set_data([state['coin2'][1]], [state['coin2'][0]])\n",
    "        else:\n",
    "            coin2_dot.set_data([-10], [-10])  # hide\n",
    "\n",
    "        title.set_text(\n",
    "            f\"Exp {state['experiment']} | Ep {state['epoch']} | Step {frame}\"\n",
    "        )\n",
    "        \n",
    "       \n",
    "        if(np.array_equal(state['action_DM'], [\"None\", \"None\"])): \n",
    "            DM_action_string = [\"None\", \"None\"]\n",
    "        else: \n",
    "            DM_action_string = f\"DM Action: [{move_action_codes[state['action_DM'][0]]}, {action_push_codes[state['action_DM'][1]]}]\"\n",
    "            \n",
    "        if(np.array_equal(state['action_DM'], [\"None\", \"None\"])): \n",
    "            Adv_action_string = [\"None\", \"None\"]\n",
    "        else: \n",
    "            Adv_action_string = f\"Adv Action: [{move_action_codes[state['action_Adv'][0]]}, {action_push_codes[state['action_Adv'][1]]}]\"\n",
    "            \n",
    "        # Update a text box the grid for step and reward info\n",
    "        info_text.set_text(\n",
    "            (\n",
    "                f\"Previous state DM: [{state['DM_location_old'][0]}, {state['DM_location_old'][1]}]\\n\"\n",
    "                f\"New State DM: [{state['DM_location_new'][0]}, {state['DM_location_new'][1]}]\\n\"\n",
    "                f\"{DM_action_string}\\n\"\n",
    "                f\"DM reward: {state['reward_DM']}\\n\"\n",
    "                f\"\\n\"  # This adds a visual separation\n",
    "                f\"Previous state Adv: [{state['Adv_location_old'][0]}, {state['Adv_location_old'][1]}]\\n\"\n",
    "                f\"New State Adv: [{state['Adv_location_new'][0]}, {state['Adv_location_new'][1]}]\\n\"\n",
    "                f\"{Adv_action_string}\\n\"\n",
    "                f\"Adv reward: {state['reward_Adv']}\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return dm_dot, adv_dot, coin1_dot, coin2_dot, title\n",
    "\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig, update,\n",
    "        frames=len(trajectory_log),\n",
    "        init_func=init,\n",
    "        blit=False,\n",
    "        repeat=False\n",
    "    )\n",
    "    ani.save(\"trajectory.mp4\", writer=\"ffmpeg\", fps=fps, dpi=dpi)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup experiment 1\n",
    "\n",
    "# Number of experiments\n",
    "N_EXP = 1\n",
    "\n",
    "# Size of single dimension of the square grid\n",
    "grid_size = 11\n",
    "\n",
    "# Initialize environment \n",
    "env = CoinGame(max_steps=1000, size_square_grid=grid_size, push_distance=2)\n",
    "\n",
    "# Number of states \n",
    "n_states = env.n_states\n",
    "\n",
    "# Number of episodes \n",
    "n_iter = 100\n",
    "\n",
    "# Constant gamma \n",
    "gamma = 0.95\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Epsilon for epsilon greedy policy\n",
    "epsilon = 0.3\n",
    "\n",
    "# Moving average window size for plotting \n",
    "moving_average_window_size = 1000\n",
    "\n",
    "r0ss = []\n",
    "r1ss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiments\n",
    "for n in range(N_EXP):\n",
    "    \n",
    "    ## Initialize agents\n",
    "    # TODO: Add reset method to individual agents and move them to initialization\n",
    "    #* Note: The agent's here are reinitialized, instead reset can be done by reseting the Q matrix inside the agent.\n",
    "\n",
    "    # TODO: Change the actions pace to something more general\n",
    "    P2 = IndQLearningAgentSoftmax(np.array(range(8)), n_states, learning_rate=learning_rate,\n",
    "                           epsilon=epsilon, gamma=gamma, enemy_action_space=None, beta = 1)\n",
    "    P1 = IndQLearningAgentSoftmax(np.array(range(8)), n_states, learning_rate=learning_rate,\n",
    "                           epsilon=epsilon, gamma=gamma, enemy_action_space=None, beta = 1)\n",
    "\n",
    "    # P2 = ExpSmoother(env.available_actions_Adv, env.available_actions_Adv, learning_rate=0.7)\n",
    "    # P2 = RandomAgent( env.available_actions, p=0.5)\n",
    "    # P1 = Level2QAgentSoftmax(env.available_actions_DM, env.available_actions_Adv, n_states=n_states,\n",
    "    #                   learning_rate=.9, epsilon=0.1, gamma=gamma)\n",
    "\n",
    "    \n",
    "    # Reset the reward vectors for the experiment\n",
    "    r0s = []\n",
    "    r1s = []\n",
    "    \n",
    "    # Run through episodes\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        # Initialize the end of episode flag\n",
    "        done = False\n",
    "        # Reset the environment after each episode\n",
    "        env.reset()\n",
    "        # Get the initial state\n",
    "        s = env.get_state()\n",
    "        \n",
    "        # Initialize cummulative observed rewards for this episode\n",
    "        episode_rewards_DM = 0\n",
    "        episode_rewards_Adv = 0\n",
    "        \n",
    "        # While the agents have not reached the terminal state\n",
    "        while not done:\n",
    "            # Agents choose actions \n",
    "            # * Note: They choose the actions simultaneously\n",
    "            a1 = P1.act(s)\n",
    "            a2 = P2.act(s)\n",
    "\n",
    "            # Transition to next time step\n",
    "            s_new, rewards, done =  env.step([a1,a2])\n",
    "\n",
    "            # Agents update their Q/Value functions\n",
    "            # * Note: They update their Q/Value functions simultaneously\n",
    "            P1.update(s, [a1, a2], [rewards[0], rewards[1]], s_new)\n",
    "            P2.update(s, [a2, a1], [rewards[1], rewards[0]], s_new)\n",
    "            \n",
    "            # Set the current state to the new state\n",
    "            s = s_new  \n",
    "            \n",
    "            # Add the observed reward to the episode reward of both agents \n",
    "            # * Note: The rewards are observed simultaneously\n",
    "            episode_rewards_DM += rewards[0]\n",
    "            episode_rewards_Adv += rewards[1]\n",
    "            \n",
    "        # Append the episode rewards to the list of rewards for this experiment\n",
    "        r0s.append(episode_rewards_DM)\n",
    "        # Append the episode rewards to the list of rewards for this experiment\n",
    "        r1s.append(episode_rewards_Adv)\n",
    "        \n",
    "        env.reset()\n",
    "        # if i%10 == 0:\n",
    "        #     P1.epsilon *= 0.995\n",
    "        #     P2.epsilon *= 0.995\n",
    "    \n",
    "          \n",
    "        \n",
    "    print(n)\n",
    "    r0ss.append(r0s)\n",
    "    r1ss.append(r1s)\n",
    "    \n",
    "plot(r0ss, r1ss, moving_average_window_size=moving_average_window_size, dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r0ss = []\n",
    "r1ss = []\n",
    "\n",
    "# Total trajectory logs for all experiments\n",
    "trajectory_logs_all_experiments = []\n",
    "\n",
    "# Run the experiments\n",
    "for n in range(N_EXP):\n",
    "    \n",
    "    ## Initialize agents\n",
    "    # TODO: Add reset method to individual agents and move them to initialization\n",
    "    #* Note: The agent's here are reinitialized, instead reset can be done by reseting the Q matrix inside the agent.\n",
    "\n",
    "    P2 = IndQLearningAgentSoftmax(np.array(range(8)), n_states, learning_rate=learning_rate,\n",
    "                           epsilon=epsilon, gamma=gamma, enemy_action_space=None, beta = 1)\n",
    "    # P1 = IndQLearningAgentSoftmax(env.available_actions_DM, n_states, learning_rate=.5,\n",
    "    #                       epsilon=0.1, gamma=gamma, enemy_action_space=None)\n",
    "\n",
    "    # P2 = ExpSmoother(env.available_actions_Adv, env.available_actions_Adv, learning_rate=0.7)\n",
    "    # P2 = RandomAgent( env.available_actions, p=0.5)\n",
    "    \n",
    "    P1 = Level2QAgent(np.array(range(8)), np.array(range(8)), n_states=n_states,\n",
    "                            learning_rate=learning_rate, epsilon=epsilon, gamma=gamma)\n",
    "\n",
    "    # P1 = Level2QAgentSoftmax(np.array(range(8)), np.array(range(8)), n_states=n_states,\n",
    "    #                     learning_rate=learning_rate, epsilon=epsilon, gamma=gamma, beta=2)\n",
    "    \n",
    "    # Reset the reward vectors for the experiment\n",
    "    r0s = []\n",
    "    r1s = []\n",
    "    \n",
    "    # Trajectory logs for visualization\n",
    "    trajectory_logs_single_experiment = []\n",
    "    \n",
    "    # Run through episodes\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        # Initialize the end of episode flag\n",
    "        done = False\n",
    "        # Reset the environment after each episode\n",
    "        env.reset()\n",
    "        # Get the initial state\n",
    "        s = env.get_state()\n",
    "        # Trajectory log for this episode\n",
    "        trajectory_log_single_epoch = []\n",
    "        \n",
    "        # Log the initial state immediately after env.reset()\n",
    "        trajectory_log_single_epoch.append({\n",
    "            'DM_location_old': [\"None\", \"None\"],\n",
    "            'Adv_location_old': [\"None\", \"None\"],\n",
    "            'DM_location_new': env.blue_player.copy(),\n",
    "            'Adv_location_new': env.red_player.copy(),\n",
    "            'coin1': env.coin_1.copy() if env.coin1_available else None,\n",
    "            'coin2': env.coin_2.copy() if env.coin2_available else None, \n",
    "            'action_DM': [\"None\", \"None\"],\n",
    "            'action_Adv': [\"None\", \"None\"],\n",
    "            'reward_DM': None,\n",
    "            'reward_Adv': None,\n",
    "            'state': s, \n",
    "            'experiment': n,\n",
    "            'epoch': i\n",
    "        })\n",
    "\n",
    "        # Initialize cummulative observed rewards for this episode\n",
    "        episode_rewards_DM = 0\n",
    "        episode_rewards_Adv = 0\n",
    "        \n",
    "        # While the agents have not reached the terminal state\n",
    "        while not done:\n",
    "            # Agents choose actions \n",
    "            # * Note: They choose the actions simultaneously\n",
    "            a1 = P1.act(s)\n",
    "            a2 = P2.act(s)\n",
    "            \n",
    "            # Save locations for logging before a step is made\n",
    "            DM_location_old = env.blue_player.copy()\n",
    "            Adv_location_old = env.red_player.copy()\n",
    "            \n",
    "            # Transition to next time step\n",
    "            s_new, rewards, done =  env.step([a1,a2])\n",
    "\n",
    "            # Agents update their Q/Value functions\n",
    "            # * Note: They update their Q/Value functions simultaneously\n",
    "            P1.update(s, [a1, a2], [rewards[0], rewards[1]], s_new)\n",
    "            P2.update(s, [a2, a1], [rewards[1], rewards[0]], s_new)\n",
    "            \n",
    "            # Set the current state to the new state\n",
    "            s = s_new  \n",
    "            \n",
    "            # Add the observed reward to the episode reward of both agents \n",
    "            # * Note: The rewards are observed simultaneously\n",
    "            episode_rewards_DM += rewards[0]\n",
    "            episode_rewards_Adv += rewards[1]\n",
    "        \n",
    "            # Log state in single episode\n",
    "            trajectory_log_single_epoch.append({\n",
    "                'DM_location_old': DM_location_old,\n",
    "                'Adv_location_old': Adv_location_old,\n",
    "                'DM_location_new': env.blue_player.copy(),\n",
    "                'Adv_location_new': env.red_player.copy(),\n",
    "                'coin1': env.coin_1.copy() if env.coin1_available else None,\n",
    "                'coin2': env.coin_2.copy() if env.coin2_available else None, \n",
    "                'action_DM': env.combined_actions_blue[a1],\n",
    "                'action_Adv': env.combined_actions_red[a2],\n",
    "                'reward_DM': rewards[0],\n",
    "                'reward_Adv': rewards[1],\n",
    "                'experiment': n,\n",
    "                'epoch': i\n",
    "            })\n",
    "            \n",
    "        # Append the episode rewards to the list of rewards for this experiment\n",
    "        r0s.append(episode_rewards_DM)\n",
    "        # Append the episode rewards to the list of rewards for this experiment\n",
    "        r1s.append(episode_rewards_Adv)\n",
    "        \n",
    "        # Append the trajectory log for this epoch to experiment log \n",
    "        trajectory_logs_single_experiment.append(trajectory_log_single_epoch)\n",
    "        \n",
    "    # Append the trajectory logs of this experiment to the total logs\n",
    "    trajectory_logs_all_experiments.append(trajectory_logs_single_experiment)\n",
    "    \n",
    "    env.reset() # Reset the environment at the very end\n",
    "    \n",
    "    print(n)\n",
    "    r0ss.append(r0s)\n",
    "    r1ss.append(r1s)\n",
    "\n",
    "\n",
    "plot(r0ss, r1ss, moving_average_window_size=moving_average_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_trajectory_from_log(trajectory_logs_all_experiments[0][99][:len(trajectory_logs_all_experiments[0][99])], grid_size=grid_size, fps=2, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"trajectory.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"trajectory.mp4\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "TMDP_DP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
