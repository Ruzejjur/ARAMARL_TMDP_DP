{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T16:26:31.229451Z",
     "start_time": "2018-12-27T16:26:31.008474Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruzejjur/Github/ARAMARL_TMDP_DP/TMDP_DP/agent_DP.py:20: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Importing custom modules\n",
    "from engine_DP import CoinGame\n",
    "from agent_DP import RandomAgent, IndQLearningAgentSoftmax, Level2QAgentSoftmax, Level2QAgent_fixed\n",
    "# from agent_DP import ExpSmoother, FPLearningAgent, Level2QAgent, RandomAgent, IndQLearningAgent, Level3QAgent, Level3QAgentMixExp, Level3QAgentMixDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T16:26:31.342933Z",
     "start_time": "2018-12-27T16:26:31.329438Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moving_average(a, n=3):\n",
    "    \"\"\"\n",
    "    Compute the simple moving average of a 1D array.\n",
    "\n",
    "    Parameters:\n",
    "        a (array-like): Input array containing numerical data.\n",
    "        n (int, optional): Window size for the moving average. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Array of moving averages with length len(a) - n + 1.\n",
    "    \"\"\"\n",
    "    # Compute cumulative sum of the input array as float\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    \n",
    "    # Subtract the cumulative sum shifted by 'n' to get the sum over each window\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    \n",
    "    # Divide by window size to obtain the moving average\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "def plot(r0ss, r1ss, moving_average_window_size=1000, dir=None):\n",
    "    \"\"\"\n",
    "    Plot smoothed reward trajectories for two agents over multiple experiments.\n",
    "\n",
    "    Parameters:\n",
    "        r0ss (list of arrays): Rewards for Agent A across experiments.\n",
    "        r1ss (list of arrays): Rewards for Agent B across experiments.\n",
    "        dir (str, optional): If provided, saves the plot to 'dir.png'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Apply 'ggplot' style for aesthetics\n",
    "    plt.style.use('ggplot')\n",
    "\n",
    "    N_EXP = len(r0ss)  # Number of experiments\n",
    "\n",
    "    # Plot individual experiment trajectories with low opacity\n",
    "    for i in range(N_EXP):\n",
    "        plt.plot(moving_average(r0ss[i], moving_average_window_size), 'b', alpha=0.05)\n",
    "        plt.plot(moving_average(r1ss[i], moving_average_window_size), 'r', alpha=0.05)\n",
    "\n",
    "    # Plot average trajectory across experiments with higher opacity\n",
    "    plt.plot(moving_average(np.mean(r0ss, axis=0), moving_average_window_size), 'b', alpha=0.5)\n",
    "    plt.plot(moving_average(np.mean(r1ss, axis=0), moving_average_window_size), 'r', alpha=0.5)\n",
    "\n",
    "    # Label axes\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('R')\n",
    "\n",
    "    # Create custom legend\n",
    "    custom_lines = [Line2D([0], [0], color='b', label='Agent A'),\n",
    "                    Line2D([0], [0], color='r', label='Agent B')]\n",
    "    plt.legend(handles=custom_lines)\n",
    "\n",
    "    # Save plot if directory is specified\n",
    "    if dir is not None:\n",
    "        plt.savefig(f\"{dir}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup experiment 1\n",
    "\n",
    "# Number of experiments\n",
    "N_EXP = 5\n",
    "\n",
    "# Size of single dimension of the square grid\n",
    "grid_size = 11\n",
    "\n",
    "# Initialize environment \n",
    "env = CoinGame(max_steps=10000000000000, size_square_grid=grid_size, push_distance=2)\n",
    "\n",
    "# Number of states \n",
    "n_states = env.n_states\n",
    "\n",
    "# Number of episodes \n",
    "n_iter = 1000\n",
    "\n",
    "# Constant gamma \n",
    "gamma = 0.95\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Epsilon for epsilon greedy policy\n",
    "epsilon = 0.3\n",
    "\n",
    "# Moving average window size for plotting \n",
    "moving_average_window_size = 100\n",
    "\n",
    "r0ss = []\n",
    "r1ss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m     40\u001b[39m     \u001b[38;5;66;03m# Agents choose actions \u001b[39;00m\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# * Note: They choose the actions simultaneously\u001b[39;00m\n\u001b[32m     42\u001b[39m     a1 = P1.act(s)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     a2 = \u001b[43mP2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# Transition to next time step\u001b[39;00m\n\u001b[32m     46\u001b[39m     s_new, rewards, done =  env.step([a1,a2])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/ARAMARL_TMDP_DP/TMDP_DP/agent_DP.py:93\u001b[39m, in \u001b[36mIndQLearningAgentSoftmax.act\u001b[39m\u001b[34m(self, obs)\u001b[39m\n\u001b[32m     91\u001b[39m p = np.exp(\u001b[38;5;28mself\u001b[39m.Q[obs,:])\n\u001b[32m     92\u001b[39m p = p / np.sum(p)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy/random/mtrand.pyx:975\u001b[39m, in \u001b[36mnumpy.random.mtrand.RandomState.choice\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/TMDP_DP/lib/python3.13/site-packages/numpy/_core/getlimits.py:493\u001b[39m, in \u001b[36mfinfo.__new__\u001b[39m\u001b[34m(cls, dtype)\u001b[39m\n\u001b[32m    489\u001b[39m _finfo_cache = {}\n\u001b[32m    491\u001b[39m __class_getitem__ = \u001b[38;5;28mclassmethod\u001b[39m(types.GenericAlias)\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, dtype):\n\u001b[32m    494\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    495\u001b[39m         obj = \u001b[38;5;28mcls\u001b[39m._finfo_cache.get(dtype)  \u001b[38;5;66;03m# most common path\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run the experiments\n",
    "for n in range(N_EXP):\n",
    "    \n",
    "    ## Initialize agents\n",
    "    # TODO: Add reset method to individual agents and move them to initialization\n",
    "    #* Note: The agent's here are reinitialized, instead reset can be done by reseting the Q matrix inside the agent.\n",
    "\n",
    "    # TODO: Change the actions pace to something more general\n",
    "    P2 = IndQLearningAgentSoftmax(np.array(range(8)), n_states, learning_rate=learning_rate,\n",
    "                           epsilon=epsilon, gamma=gamma, enemy_action_space=None)\n",
    "    P1 = IndQLearningAgentSoftmax(np.array(range(8)), n_states, learning_rate=learning_rate,\n",
    "                           epsilon=epsilon, gamma=gamma, enemy_action_space=None)\n",
    "\n",
    "    # P2 = ExpSmoother(env.available_actions_Adv, env.available_actions_Adv, learning_rate=0.7)\n",
    "    # P2 = RandomAgent( env.available_actions, p=0.5)\n",
    "    # P1 = Level2QAgentSoftmax(env.available_actions_DM, env.available_actions_Adv, n_states=n_states,\n",
    "    #                   learning_rate=.9, epsilon=0.1, gamma=gamma)\n",
    "\n",
    "    \n",
    "    # Reset the reward vectors for the experiment\n",
    "    r0s = []\n",
    "    r1s = []\n",
    "    \n",
    "    # Run through episodes\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        # Initialize the end of episode flag\n",
    "        done = False\n",
    "        # Reset the environment after each episode\n",
    "        env.reset()\n",
    "        # Get the initial state\n",
    "        s = env.get_state()\n",
    "        \n",
    "        # Initialize cummulative observed rewards for this episode\n",
    "        episode_rewards_DM = 0\n",
    "        episode_rewards_Adv = 0\n",
    "        \n",
    "        # While the agents have not reached the terminal state\n",
    "        while not done:\n",
    "            # Agents choose actions \n",
    "            # * Note: They choose the actions simultaneously\n",
    "            a1 = P1.act(s)\n",
    "            a2 = P2.act(s)\n",
    "\n",
    "            # Transition to next time step\n",
    "            s_new, rewards, done =  env.step([a1,a2])\n",
    "\n",
    "            # Agents update their Q/Value functions\n",
    "            # * Note: They update their Q/Value functions simultaneously\n",
    "            P1.update(s, [a1, a2], [rewards[0], rewards[1]], s_new)\n",
    "            P2.update(s, [a2, a1], [rewards[1], rewards[0]], s_new)\n",
    "            \n",
    "            # Set the current state to the new state\n",
    "            s = s_new  \n",
    "            \n",
    "            # Add the observed reward to the episode reward of both agents \n",
    "            # * Note: The rewards are observed simultaneously\n",
    "            episode_rewards_DM += rewards[0]\n",
    "            episode_rewards_Adv += rewards[1]\n",
    "            \n",
    "        # Append the episode rewards to the list of rewards for this experiment\n",
    "        r0s.append(episode_rewards_DM)\n",
    "        # Append the episode rewards to the list of rewards for this experiment\n",
    "        r1s.append(episode_rewards_Adv)\n",
    "        \n",
    "        env.reset()\n",
    "        # if i%10 == 0:\n",
    "        #     P1.epsilon *= 0.995\n",
    "        #     P2.epsilon *= 0.995\n",
    "    \n",
    "          \n",
    "        \n",
    "    print(n)\n",
    "    r0ss.append(r0s)\n",
    "    r1ss.append(r1s)\n",
    "    \n",
    "plot(r0ss, r1ss, moving_average_window_size=moving_average_window_size, dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r0ss = []\n",
    "r1ss = []\n",
    "\n",
    "\n",
    "# Run the experiments\n",
    "for n in range(N_EXP):\n",
    "    \n",
    "    ## Initialize agents\n",
    "    # TODO: Add reset method to individual agents and move them to initialization\n",
    "    #* Note: The agent's here are reinitialized, instead reset can be done by reseting the Q matrix inside the agent.\n",
    "\n",
    "    P2 = IndQLearningAgentSoftmax(np.array(range(8)), n_states, learning_rate=learning_rate,\n",
    "                           epsilon=epsilon, gamma=gamma, enemy_action_space=None)\n",
    "    # P1 = IndQLearningAgentSoftmax(env.available_actions_DM, n_states, learning_rate=.5,\n",
    "    #                       epsilon=0.1, gamma=gamma, enemy_action_space=None)\n",
    "\n",
    "    # P2 = ExpSmoother(env.available_actions_Adv, env.available_actions_Adv, learning_rate=0.7)\n",
    "    # P2 = RandomAgent( env.available_actions, p=0.5)\n",
    "    # P1 = Level2QAgentSoftmax(env.available_actions_DM, env.available_actions_Adv, n_states=n_states,\n",
    "    #                       learning_rate=learning_rate, epsilon=epsilon, gamma=gamma)\n",
    "    \n",
    "    P1 = Level2QAgent_fixed(np.array(range(8)), np.array(range(8)), n_states=n_states,\n",
    "                            learning_rate=learning_rate, epsilon=epsilon, gamma=gamma)\n",
    "\n",
    "    \n",
    "    # Reset the reward vectors for the experiment\n",
    "    r0s = []\n",
    "    r1s = []\n",
    "    \n",
    "    # Run through episodes\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        # Initialize the end of episode flag\n",
    "        done = False\n",
    "        # Reset the environment after each episode\n",
    "        env.reset()\n",
    "        # Get the initial state\n",
    "        s = env.get_state()\n",
    "        \n",
    "        # Initialize cummulative observed rewards for this episode\n",
    "        episode_rewards_DM = 0\n",
    "        episode_rewards_Adv = 0\n",
    "        \n",
    "        # While the agents have not reached the terminal state\n",
    "        while not done:\n",
    "            # Agents choose actions \n",
    "            # * Note: They choose the actions simultaneously\n",
    "            a1 = P1.act(s)\n",
    "            a2 = P2.act(s)\n",
    "\n",
    "            # Transition to next time step\n",
    "            s_new, rewards, done =  env.step([a1,a2])\n",
    "\n",
    "            # Agents update their Q/Value functions\n",
    "            # * Note: They update their Q/Value functions simultaneously\n",
    "            P1.update(s, [a1, a2], [rewards[0], rewards[1]], s_new)\n",
    "            P2.update(s, [a2, a1], [rewards[1], rewards[0]], s_new)\n",
    "            \n",
    "            # Set the current state to the new state\n",
    "            s = s_new  \n",
    "            \n",
    "            # Add the observed reward to the episode reward of both agents \n",
    "            # * Note: The rewards are observed simultaneously\n",
    "            episode_rewards_DM += rewards[0]\n",
    "            episode_rewards_Adv += rewards[1]\n",
    "            \n",
    "        # Append the episode rewards to the list of rewards for this experiment\n",
    "        r0s.append(episode_rewards_DM)\n",
    "        # Append the episode rewards to the list of rewards for this experiment\n",
    "        r1s.append(episode_rewards_Adv)\n",
    "        \n",
    "        # if i%10 == 0:\n",
    "        #     P1.epsilonA *= 0.995\n",
    "        #     P2.epsilon *= 0.995\n",
    "        #     #P1.epsilonB *= 0.9\n",
    "    \n",
    "    env.reset() # Reset the environment at the very end\n",
    "    \n",
    "    print(n)\n",
    "    r0ss.append(r0s)\n",
    "    r1ss.append(r1s)\n",
    "\n",
    "\n",
    "plot(r0ss, r1ss, moving_average_window_size=moving_average_window_size)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "TMDP_DP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
