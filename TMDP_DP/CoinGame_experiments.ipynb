{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa22979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import yaml\n",
    "from pprint import pprint  # For better printing of the config dictionary\n",
    "from IPython.display import Video, display\n",
    "\n",
    "# For interactive plots in .ipynb scripts\n",
    "%matplotlib widget\n",
    "\n",
    "from experiment_runner import run_experiment\n",
    "from utils.trajectory_log_schema import TRAJECTORY_LOG_COLUMN_MAP \n",
    "from utils.plot_utils import animate_trajectory_from_log, plot_reward_per_episode_series, plot_result_ration\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    stream=sys.stdout,\n",
    "    force=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc40fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration and run the experiment\n",
    "#config_file_path = 'configs/config.yaml'\n",
    "\n",
    "config_file_path = '/Users/ruzejjur/Github/ARAMARL_TMDP_DP/TMDP_DP/configs/config_new_trajec_save_test.yaml'\n",
    "\n",
    "\n",
    "results_path = run_experiment(config_file_path, log_trajectory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e322763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the Experiment Run to Analyze\n",
    "# ---------------------------------------------\n",
    "# TODO: The path can be set to the specific, timestamped experiment folder you want to analyze.\n",
    "\n",
    "# --- Select moving average window size and range for the player reward plots\n",
    "\n",
    "plot_moving_average_window_size = 400\n",
    "reward_time_series_x_axis_plot_range = [0,19999]\n",
    "\n",
    "episode_range_to_eval = [19899,19999] # Checking the win, loss and draw ratio dynamic for last 100 episodes\n",
    "game_result_ratio_x_axis_plot_range = [19899,19999]\n",
    "\n",
    "\n",
    "# --- Select which trajectory to animate ---\n",
    "# The log is structured as: [experiment_run][episode][step]\n",
    "run_to_animate = 9      # The last experiment run\n",
    "episode_to_animate = 19999 # The last episode of that run\n",
    "\n",
    "#results_run_directory = results_path\n",
    "\n",
    "results_run_directory = '/Users/ruzejjur/Github/ARAMARL_TMDP_DP/TMDP_DP/results/MDP_DP_Agent_PerfectModel_vs_ManhattanAgent_Passive_2025-08-20_16-44-52'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f3e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data ---\n",
    "\n",
    "# Verify that the directory exists before proceeding\n",
    "if not os.path.isdir(results_run_directory):\n",
    "    raise FileNotFoundError(f\"The specified directory does not exist: {results_run_directory}\")\n",
    "\n",
    "logging.info(f\"Loading results from: {results_run_directory}\")\n",
    "\n",
    "# --- Load Configuration ---\n",
    "config_path = os.path.join(results_run_directory, 'config*.yaml')\n",
    "found_configs = glob.glob(config_path)\n",
    "\n",
    "config = None\n",
    "if not found_configs:\n",
    "\n",
    "    logging.error(f\"No configuration file matching 'config*.yaml' found in the directory: {results_run_directory}\")\n",
    "    raise FileNotFoundError(\"Could not find a configuration file to load.\")\n",
    "else:\n",
    "    if len(found_configs) > 1:\n",
    "        \n",
    "        logging.warning(\n",
    "            f\"Multiple configuration files found: {found_configs}. \"\n",
    "            f\"Loading the first one: '{os.path.basename(found_configs[0])}'.\"\n",
    "        )\n",
    "        \n",
    "    config_path = found_configs[0]\n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        logging.info(f\"Configuration file '{os.path.basename(config_path)}' loaded successfully.\")\n",
    "        \n",
    "        # Use pprint for a cleaner print of the configuration dictionary\n",
    "        logging.info(\"\\n--- Experiment Configuration ---\")\n",
    "        pprint(config)\n",
    "        logging.info(\"--------------------------------\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load or parse the configuration file at: {config_path}. Error: {e}\")\n",
    "        # Ensure config is None if loading fails\n",
    "        config = None\n",
    "\n",
    "\n",
    "# --- Load Reward Data ---\n",
    "\n",
    "# Loading full rewards data\n",
    "full_rewards_p0_path = os.path.join(results_run_directory, 'full_rewards_per_episode_p0.npz')\n",
    "full_rewards_p1_path = os.path.join(results_run_directory, 'full_rewards_per_episode_p1.npz')\n",
    "try:\n",
    "    # Use np.load to read the saved NumPy array files\n",
    "    data_p0 = np.load(full_rewards_p0_path)\n",
    "    data_p1 = np.load(full_rewards_p1_path)\n",
    "    \n",
    "    full_rewards_p0 = data_p0['full_rewards_p0']\n",
    "    full_rewards_p1 = data_p1['full_rewards_p1']\n",
    "    \n",
    "    logging.info(f\"\\n Full reward data loaded successfully. Shape: {full_rewards_p0.shape}\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Full reward data (.npz files) not found in the directory.\")\n",
    "\n",
    "    full_rewards_p0, full_rewards_p1 = None, None\n",
    "    \n",
    "# Loading positive rewards data\n",
    "positive_rewards_p0_path = os.path.join(results_run_directory, 'positive_rewards_per_episode_p0.npz')\n",
    "positive_rewards_p1_path = os.path.join(results_run_directory, 'positive_rewards_per_episode_p1.npz')\n",
    "try:\n",
    "    # Use np.load to read the saved NumPy array files\n",
    "    data_p0 = np.load(positive_rewards_p0_path)\n",
    "    data_p1 = np.load(positive_rewards_p1_path)\n",
    "\n",
    "    positive_rewards_p0 = data_p0['positive_rewards_p0']\n",
    "    positive_rewards_p1 = data_p1['positive_rewards_p1']\n",
    "\n",
    "    logging.info(f\"\\n Positive reward data loaded successfully. Shape: {positive_rewards_p0.shape}\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Positive reward data (.npz files) not found in the directory.\")\n",
    "\n",
    "    positive_rewards_p0, positive_rewards_p1 = None, None\n",
    "    \n",
    "# Loading negative rewards data\n",
    "negative_rewards_p0_path = os.path.join(results_run_directory, 'negative_rewards_per_episode_p0.npz')\n",
    "negative_rewards_p1_path = os.path.join(results_run_directory, 'negative_rewards_per_episode_p1.npz')\n",
    "try:\n",
    "    # Use np.load to read the saved NumPy array files\n",
    "    data_p0 = np.load(negative_rewards_p0_path)\n",
    "    data_p1 = np.load(negative_rewards_p1_path)\n",
    "    \n",
    "    negative_rewards_p0 = data_p0['negative_rewards_p0']\n",
    "    negative_rewards_p1 = data_p1['negative_rewards_p1']\n",
    "    \n",
    "    logging.info(f\"\\n Negative reward data loaded successfully. Shape: {negative_rewards_p0.shape}\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Negative reward data (.npz files) not found in the directory.\")\n",
    "    \n",
    "    negative_rewards_p0, negative_rewards_p1 = None, None\n",
    "    \n",
    "# Loading only step rewards data\n",
    "only_step_rewards_p0_path = os.path.join(results_run_directory, 'only_step_rewards_per_episode_p0.npz')\n",
    "only_step_rewards_p1_path = os.path.join(results_run_directory, 'only_step_rewards_per_episode_p1.npz')\n",
    "try:\n",
    "    # Use np.load to read the saved NumPy array files\n",
    "    data_p0 = np.load(only_step_rewards_p0_path)\n",
    "    data_p1 = np.load(only_step_rewards_p1_path)\n",
    "    \n",
    "    only_step_rewards_p0 = data_p0['only_step_rewards_p0']\n",
    "    only_step_rewards_p1 = data_p1['only_step_rewards_p1']\n",
    "    \n",
    "    logging.info(f\"\\n Only step reward data loaded successfully. Shape: {only_step_rewards_p0.shape}\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Only step reward data (.npz files) not found in the directory.\")\n",
    "    \n",
    "    only_step_rewards_p0, only_step_rewards_p1 = None, None\n",
    "    \n",
    "# Loading full rewards without coin data\n",
    "full_rewards_without_coin_p0_path = os.path.join(results_run_directory, 'full_rewards_without_coin_per_episode_p0.npz')\n",
    "full_rewards_without_coin_p1_path = os.path.join(results_run_directory, 'full_rewards_without_coin_per_episode_p1.npz')\n",
    "try:\n",
    "    # Use np.load to read the saved NumPy array files\n",
    "    data_p0 = np.load(full_rewards_without_coin_p0_path)\n",
    "    data_p1 = np.load(full_rewards_without_coin_p1_path)\n",
    "    \n",
    "    full_rewards_without_coin_p0 = data_p0['full_rewards_without_coin_p0']\n",
    "    full_rewards_without_coin_p1 = data_p1['full_rewards_without_coin_p1']\n",
    "    \n",
    "    logging.info(f\"\\n Full reward without coin data loaded successfully. Shape: {full_rewards_without_coin_p0.shape}\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Full reward without coin data (.npz files) not found in the directory.\")\n",
    "\n",
    "    full_rewards_without_coin_p0, full_rewards_without_coin_p1 = None, None\n",
    "    \n",
    "# Loading full rewards without step data\n",
    "full_rewards_without_step_p0_path = os.path.join(results_run_directory, 'full_rewards_without_step_per_episode_p0.npz')\n",
    "full_rewards_without_step_p1_path = os.path.join(results_run_directory, 'full_rewards_without_step_per_episode_p1.npz')\n",
    "try:\n",
    "    # Use np.load to read the saved NumPy array files\n",
    "    data_p0 = np.load(full_rewards_without_step_p0_path)\n",
    "    data_p1 = np.load(full_rewards_without_step_p1_path)\n",
    "    \n",
    "    full_rewards_without_step_p0 = data_p0['full_rewards_without_step_p0']\n",
    "    full_rewards_without_step_p1 = data_p1['full_rewards_without_step_p1']\n",
    "    \n",
    "    logging.info(f\"\\n Full reward without step data loaded successfully. Shape: {full_rewards_without_step_p0.shape}\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Full reward without step data (.npz files) not found in the directory.\")\n",
    "\n",
    "    full_rewards_without_step_p0, full_rewards_without_step_p1 = None, None\n",
    "    \n",
    "# Loading game result for player 1\n",
    "game_result_p0_path = os.path.join(results_run_directory, 'game_result_episode_p0.npz')\n",
    "try:\n",
    "    # Use np.load to read the saved NumPy array files\n",
    "    data_p0 = np.load(game_result_p0_path)\n",
    "    \n",
    "    game_result_p0 = data_p0['game_result_p0']\n",
    "    \n",
    "    logging.info(f\"\\n Game result for player 1 data loaded successfully. Shape: {game_result_p0.shape}\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Game result for player 1 data (.npz files) not found in the directory.\")\n",
    "\n",
    "    game_result_p0 = None\n",
    "    \n",
    "# --- Load Trajectory Data ---\n",
    "# This will only exist if log_trajectory=True was used for the run.\n",
    "trajectory_path = os.path.join(results_run_directory, 'trajectory_log.npz')\n",
    "try:\n",
    "    \n",
    "    data = np.load(trajectory_path)\n",
    "    trajectory_logs_all_experiments = data['trajectory_logs_all_experiments']\n",
    "    \n",
    "    logging.info(f\"Trajectory loaded successfully. Shape: {trajectory_logs_all_experiments.shape}\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"\\n Trajectory log ('trajectory_log.npz') not found. Was log_trajectory=True set during the run?\")\n",
    "\n",
    "    trajectory_logs_all_experiments = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4599c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Display the Reward Plot ---\n",
    "# We can either regenerate the plot from the loaded data or simply display the image that was saved.\n",
    "# Displaying the image is faster and guarantees we see the original result.\n",
    "\n",
    "# Plotting full rewards (negative + positive)\n",
    "if full_rewards_p0 is not None and full_rewards_p1 is not None:\n",
    "    logging.info(\"\\n--- Creating full reward plot from saved player rewards ---\")\n",
    "    plot_title = 'Cumulative full rewards (positive + negative) per episode'\n",
    "    \n",
    "    plot_reward_per_episode_series(full_rewards_p0, full_rewards_p1,\n",
    "                                   plot_title, episode_series_x_axis_plot_range = reward_time_series_x_axis_plot_range, moving_average_window_size=plot_moving_average_window_size, dir=None)\n",
    "else:\n",
    "    logging.info(\"\\nSkipping full rewards plot display as rewards were not not loaded.\")\n",
    "\n",
    "# Plotting only positive rewards\n",
    "if positive_rewards_p0 is not None and positive_rewards_p1 is not None:\n",
    "    logging.info(\"\\n--- Creating positive reward plot from saved player rewards ---\")\n",
    "    plot_title = 'Cumulative positive rewards per episode'\n",
    "    \n",
    "    plot_reward_per_episode_series(positive_rewards_p0, positive_rewards_p1,\n",
    "                                   plot_title, episode_series_x_axis_plot_range = reward_time_series_x_axis_plot_range, moving_average_window_size=plot_moving_average_window_size, dir=None)\n",
    "else:\n",
    "    logging.info(\"\\nSkipping positive rewards plot display as rewards were not not loaded.\")\n",
    "\n",
    "# Plotting only negative rewards\n",
    "if negative_rewards_p0 is not None and negative_rewards_p1 is not None:\n",
    "    logging.info(\"\\n--- Creating negative reward plot from saved player rewards ---\")\n",
    "    plot_title = 'Cumulative negative rewards per episode'\n",
    "    \n",
    "    plot_reward_per_episode_series(negative_rewards_p0, negative_rewards_p1,\n",
    "                                   plot_title, episode_series_x_axis_plot_range = reward_time_series_x_axis_plot_range, moving_average_window_size=plot_moving_average_window_size, dir=None)\n",
    "else:\n",
    "    logging.info(\"\\nSkipping negative rewards plot display as rewards were not not loaded.\")\n",
    "    \n",
    "# Plotting only step rewards\n",
    "if only_step_rewards_p0 is not None and only_step_rewards_p1 is not None:\n",
    "    logging.info(\"\\n--- Creating only step reward plot from saved player rewards ---\")\n",
    "    plot_title = 'Cumulative only step rewards per episode'\n",
    "    \n",
    "    plot_reward_per_episode_series(only_step_rewards_p0, only_step_rewards_p1,\n",
    "                                   plot_title, episode_series_x_axis_plot_range = reward_time_series_x_axis_plot_range, moving_average_window_size=plot_moving_average_window_size, dir=None)\n",
    "else:\n",
    "    logging.info(\"\\nSkipping only step rewards plot display as rewards were not not loaded.\")\n",
    "    \n",
    "# Plotting full rewards without coin\n",
    "if full_rewards_without_coin_p0 is not None and full_rewards_without_coin_p1 is not None:\n",
    "    logging.info(\"\\n--- Creating full reward without coin plot from saved player rewards ---\")\n",
    "    plot_title = 'Cumulative full rewards without coin per episode'\n",
    "    \n",
    "    plot_reward_per_episode_series(full_rewards_without_coin_p0, full_rewards_without_coin_p1,\n",
    "                                   plot_title, episode_series_x_axis_plot_range = reward_time_series_x_axis_plot_range, moving_average_window_size=plot_moving_average_window_size, dir=None)\n",
    "else:\n",
    "    logging.info(\"\\nSkipping full rewards without coin plot display as rewards were not not loaded.\")\n",
    "    \n",
    "# Plotting full rewards without step\n",
    "if full_rewards_without_step_p0 is not None and full_rewards_without_step_p1 is not None:\n",
    "    logging.info(\"\\n--- Creating full reward without step plot from saved player rewards ---\")\n",
    "    plot_title = 'Cumulative full rewards without step per episode'\n",
    "    \n",
    "    plot_reward_per_episode_series(full_rewards_without_step_p0, full_rewards_without_step_p1,\n",
    "                                   plot_title, episode_series_x_axis_plot_range = reward_time_series_x_axis_plot_range, moving_average_window_size=plot_moving_average_window_size, dir=None)\n",
    "else:\n",
    "    logging.info(\"\\nSkipping full rewards without step plot display as rewards were not not loaded.\")\n",
    "    \n",
    "    \n",
    "# Plotting win ratio for player 1 without step\n",
    "if game_result_p0 is not None:\n",
    "    logging.info(\"\\n--- Creating win ratio plot for player 1 ---\")\n",
    "    plot_title = 'Win ratio without step per episode'\n",
    "    \n",
    "    plot_result_ration(game_result_p0, episode_range_to_eval, plot_title, result_type_to_plot=\"win\",\n",
    "        episode_series_x_axis_plot_range=game_result_ratio_x_axis_plot_range, \n",
    "        dir=None)\n",
    "else:\n",
    "    logging.info(\"\\nSkipping Win ratio plot display as game results were not not loaded.\")\n",
    "    \n",
    "# Plotting loss ratio for player 1 without step\n",
    "if game_result_p0 is not None:\n",
    "    logging.info(\"\\n--- Creating loss ratio plot for player 1 ---\")\n",
    "    plot_title = 'Loss ratio without step per episode'\n",
    "    \n",
    "    plot_result_ration(game_result_p0, episode_range_to_eval, plot_title, result_type_to_plot=\"loss\",\n",
    "        episode_series_x_axis_plot_range=game_result_ratio_x_axis_plot_range, \n",
    "        dir=None)\n",
    "else:\n",
    "    logging.info(\"\\nSkipping Loss ratio plot display as game results were not not loaded.\")\n",
    "    \n",
    "# Plotting draw ratio for player 1 without step\n",
    "if game_result_p0 is not None:\n",
    "    logging.info(\"\\n--- Creating draw ratio plot for player 1 ---\")\n",
    "    plot_title = 'Draw ratio without step per episode'\n",
    "    \n",
    "    plot_result_ration(game_result_p0, episode_range_to_eval, plot_title, result_type_to_plot=\"draw\",\n",
    "        episode_series_x_axis_plot_range=game_result_ratio_x_axis_plot_range, \n",
    "        dir=None)\n",
    "else:\n",
    "    logging.info(\"\\nSkipping draw ratio plot display as game results were not not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7f8fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Animate a Specific Episode from the Trajectory Log ---\n",
    "if trajectory_logs_all_experiments is not None and config is not None:\n",
    "    logging.info(\"\\n--- Generating Animation from Trajectory Log ---\")\n",
    "    \n",
    "    try:\n",
    "        # Select the data for the desired run and episode\n",
    "        num_runs = int(np.max(trajectory_logs_all_experiments['experiment_num'])) + 1\n",
    "        num_episodes = int(np.max(trajectory_logs_all_experiments['episode_num'])) + 1\n",
    "        \n",
    "        target_run = run_to_animate if run_to_animate >= 0 else num_runs + run_to_animate\n",
    "        target_episode = episode_to_animate if episode_to_animate >= 0 else num_episodes + episode_to_animate\n",
    "\n",
    "        # Select desired run and episode to animate using boolean masking\n",
    "        run_mask = trajectory_logs_all_experiments['experiment_num'] == target_run\n",
    "        episode_mask = trajectory_logs_all_experiments['episode_num'] == target_episode\n",
    "        \n",
    "        episode_data_slice = trajectory_logs_all_experiments[run_mask & episode_mask]\n",
    "        \n",
    "        # Sort by step number to ensure correct animation order\n",
    "        episode_data_slice = episode_data_slice[episode_data_slice['step_num'].argsort()]\n",
    "        \n",
    "        if episode_data_slice.shape[0] == 0:\n",
    "             raise IndexError(f\"No data found for run {target_run}, episode {target_episode}. Check your indices.\")\n",
    "        \n",
    "        grid_size = config['environment_settings']['params']['grid_size']\n",
    "        \n",
    "        # Call the animation function\n",
    "        animate_trajectory_from_log(\n",
    "            trajectory_episode_array=episode_data_slice,\n",
    "            grid_size=grid_size, \n",
    "            fps=2\n",
    "        )\n",
    "        logging.info(\"Animation saved as 'trajectory.mp4'\")\n",
    "        \n",
    "        display(Video(url=\"trajectory.mp4?cache=\" + str(time.time()), embed=False))\n",
    "        \n",
    "    except (IndexError, TypeError) as e:\n",
    "        logging.error(f\"Could not extract the specified trajectory log. Error: {e}\")\n",
    "\n",
    "else:\n",
    "    logging.info(\"\\\\nSkipping animation as trajectory data or config was not loaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TMDP_DP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
