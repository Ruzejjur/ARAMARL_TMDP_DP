{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T16:26:31.229451Z",
     "start_time": "2018-12-27T16:26:31.008474Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from engine import AdvRw2, AdvRwGridworld, CoinGame\n",
    "from agent import ExpSmoother, FPLearningAgent, Level2QAgent, RandomAgent, IndQLearningAgent\n",
    "from agent_differentiable import RegressionIndQLearningSoftmax\n",
    "from agent import Level3QAgent, Level3QAgentMixExp, Level3QAgentMixDir, IndQLearningAgentSoftmax, Level2QAgentSoftmax\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T16:26:31.342933Z",
     "start_time": "2018-12-27T16:26:31.329438Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "def plot(r0ss, r1ss, dir=None):\n",
    "    # We set a fancy theme\n",
    "    plt.style.use('ggplot')\n",
    "    N_EXP = len(r0ss)\n",
    "    #plt.axis([0, max_steps*n_iter, -4.5, 1.5])\n",
    "    for i in range(N_EXP):\n",
    "        plt.plot(moving_average(r0ss[i], 1000), 'b', alpha=0.05)\n",
    "        plt.plot(moving_average(r1ss[i], 1000), 'r', alpha=0.05)\n",
    "\n",
    "    plt.plot(moving_average(np.asarray(r0ss).mean(axis=0), 1000), 'b', alpha=0.5)\n",
    "    plt.plot(moving_average(np.asarray(r1ss).mean(axis=0), 1000), 'r', alpha=0.5)\n",
    "\n",
    "    plt.xlabel('t');\n",
    "    plt.ylabel('R');\n",
    "\n",
    "    from matplotlib.lines import Line2D\n",
    "    cmap = plt.cm.coolwarm\n",
    "    custom_lines = [Line2D([0], [0], color='b'),\n",
    "                    Line2D([0], [0], color='r')]\n",
    "\n",
    "    #plt.legend(custom_lines,['Agent A']);\n",
    "    if dir is not None:\n",
    "        plt.savefig(dir + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 845888 is out of bounds for axis 0 with size 6561",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     39\u001b[39m episode_rewards_Adv = \u001b[32m0\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m     42\u001b[39m \n\u001b[32m     43\u001b[39m \n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# Agents decide\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     a1 = \u001b[43mP1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m     a2 = P2.act(s)\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# This is to test the neutral adversary\u001b[39;00m\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m#a2 = np.random.choice(2, p=[0.9, 0.1])\u001b[39;00m\n\u001b[32m     49\u001b[39m \n\u001b[32m     50\u001b[39m \n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m# World changes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/ARAMARL_TMDP_DP/agent.py:104\u001b[39m, in \u001b[36mIndQLearningAgentSoftmax.act\u001b[39m\u001b[34m(self, obs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     p = np.exp(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    105\u001b[39m     p = p / np.sum(p)\n\u001b[32m    106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m choice(\u001b[38;5;28mself\u001b[39m.action_space, p=p)\n",
      "\u001b[31mIndexError\u001b[39m: index 845888 is out of bounds for axis 0 with size 6561"
     ]
    }
   ],
   "source": [
    "N_EXP = 5\n",
    "\n",
    "r0ss = []\n",
    "r1ss = []\n",
    "#\n",
    "for n in range(N_EXP):\n",
    "\n",
    "    gamma = 0.99\n",
    "\n",
    "    env = CoinGame(max_steps=12)\n",
    "    n_states = (3*3)**4\n",
    "    ##\n",
    "    # P2 = ExpSmoother(env.available_actions_Adv, env.available_actions_Adv, learning_rate=0.7)\n",
    "    P2 = RandomAgent( env.available_actions, p=0.5)\n",
    "    \n",
    "    P2 = IndQLearningAgentSoftmax(env.available_actions_Adv, n_states, learning_rate=.5,\n",
    "                           epsilon=0.1, gamma=gamma, enemy_action_space=None)\n",
    "    P1 = IndQLearningAgentSoftmax(env.available_actions_DM, n_states, learning_rate=.5,\n",
    "                           epsilon=0.1, gamma=gamma, enemy_action_space=None)\n",
    "    #P1 = Level2QAgentSoftmax(env.available_actions_DM, env.available_actions_Adv, n_states=n_states,\n",
    "    #                   learning_rate=.9, epsilon=0.1, gamma=gamma)\n",
    "\n",
    "    \n",
    "    n_iter = 10000        \n",
    "\n",
    "\n",
    "    r0s = []\n",
    "    r1s = []\n",
    "    \n",
    "    \n",
    "    for i in range(n_iter):\n",
    "\n",
    "        # A full episode:\n",
    "        done = False\n",
    "        env.reset()\n",
    "        s = env.get_state()\n",
    "        \n",
    "        episode_rewards_DM = 0\n",
    "        episode_rewards_Adv = 0\n",
    "            \n",
    "        while not done:\n",
    "            \n",
    "\n",
    "            # Agents decide\n",
    "            a1 = P1.act(s)\n",
    "            a2 = P2.act(s)\n",
    "            # This is to test the neutral adversary\n",
    "            #a2 = np.random.choice(2, p=[0.9, 0.1])\n",
    "\n",
    "\n",
    "            # World changes\n",
    "            s_new, rewards, done =  env.step([a1,a2])\n",
    "\n",
    "\n",
    "            # Agents learn\n",
    "\n",
    "            P1.update(s, [a1, a2], [rewards[0], rewards[1]], s_new)\n",
    "            P2.update(s, [a2, a1], [rewards[1], rewards[0]], s_new)\n",
    "            \n",
    "            s = s_new  \n",
    "            #print(r0, r1)\n",
    "            episode_rewards_DM += rewards[0]\n",
    "            episode_rewards_Adv += rewards[1]\n",
    "            \n",
    "            \n",
    "        r0s.append(episode_rewards_DM)\n",
    "        r1s.append(episode_rewards_Adv)\n",
    "        env.reset()\n",
    "        if i%10 == 0:\n",
    "            P1.epsilon *= 0.995\n",
    "            P2.epsilon *= 0.995\n",
    "            #P1.epsilonB *= 0.9\n",
    "        \n",
    "    print(n)\n",
    "    r0ss.append(r0s)\n",
    "    r1ss.append(r1s)\n",
    "    \n",
    "plot(r0ss, r1ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruzejjur/Github/ARAMARL_TMDP_DP/agent.py:22: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 845888 is out of bounds for axis 0 with size 6561",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     44\u001b[39m episode_rewards_Adv = \u001b[32m0\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m     47\u001b[39m \n\u001b[32m     48\u001b[39m \n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# Agents decide\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     a1 = \u001b[43mP1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     a2 = P2.act(s)\n\u001b[32m     52\u001b[39m     \u001b[38;5;66;03m# This is to test the neutral adversary\u001b[39;00m\n\u001b[32m     53\u001b[39m     \u001b[38;5;66;03m#a2 = np.random.choice(2, p=[0.9, 0.1])\u001b[39;00m\n\u001b[32m     54\u001b[39m \n\u001b[32m     55\u001b[39m \n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# World changes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/ARAMARL_TMDP_DP/agent.py:798\u001b[39m, in \u001b[36mLevel2QAgentSoftmax.act\u001b[39m\u001b[34m(self, obs)\u001b[39m\n\u001b[32m    797\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m798\u001b[39m     b = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menemy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    799\u001b[39m     p = np.exp(\u001b[38;5;28mself\u001b[39m.QA[obs,:,b])\n\u001b[32m    800\u001b[39m     p = p / np.sum(p)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/ARAMARL_TMDP_DP/agent.py:269\u001b[39m, in \u001b[36mFPLearningAgent.act\u001b[39m\u001b[34m(self, obs)\u001b[39m\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m choice(\u001b[38;5;28mself\u001b[39m.action_space)\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    265\u001b[39m     \u001b[38;5;66;03m#print('obs ', obs)\u001b[39;00m\n\u001b[32m    266\u001b[39m     \u001b[38;5;66;03m#print(self.Q[obs].shape)\u001b[39;00m\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m#print(self.Dir.shape)\u001b[39;00m\n\u001b[32m    268\u001b[39m     \u001b[38;5;66;03m#print(np.dot( self.Q[obs], self.Dir/np.sum(self.Dir) ).shape)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.action_space[ np.argmax( np.dot( \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;28mself\u001b[39m.Dir/np.sum(\u001b[38;5;28mself\u001b[39m.Dir) ) ) ]\n",
      "\u001b[31mIndexError\u001b[39m: index 845888 is out of bounds for axis 0 with size 6561"
     ]
    }
   ],
   "source": [
    "N_EXP = 5\n",
    "\n",
    "r0ss = []\n",
    "r1ss = []\n",
    "#\n",
    "for n in range(N_EXP):\n",
    "\n",
    "    gamma = 0.99\n",
    "\n",
    "    env = CoinGame(max_steps=12)\n",
    "    n_states = (3*3)**4\n",
    "    ##\n",
    "    # P2 = ExpSmoother(env.available_actions_Adv, env.available_actions_Adv, learning_rate=0.7)\n",
    "    # P2 = RandomAgent( env.available_actions, p=0.5)\n",
    "    \n",
    "    # P2 = IndQLearningAgentSoftmax(env.available_actions_Adv, n_states, learning_rate=.5,\n",
    "    #                        epsilon=0.1, gamma=gamma, enemy_action_space=None)\n",
    "    \n",
    "    P2 = Level2QAgentSoftmax(env.available_actions_DM, env.available_actions_Adv, n_states=n_states,\n",
    "                       learning_rate=.5, epsilon=0.1, gamma=gamma)\n",
    "    #P1 = IndQLearningAgent(env.available_actions_DM, n_states, learning_rate=0.5,\n",
    "    #                       epsilon=0.1, gamma=gamma, enemy_action_space=None)\n",
    "    P1 = Level2QAgentSoftmax(env.available_actions_DM, env.available_actions_Adv, n_states=n_states,\n",
    "                       learning_rate=.5, epsilon=0.1, gamma=gamma)\n",
    "\n",
    "    \n",
    "    n_iter = 10000        \n",
    "\n",
    "\n",
    "    r0s = []\n",
    "    r1s = []\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        # A full episode:\n",
    "        done = False\n",
    "        env.reset()\n",
    "        s = env.get_state()\n",
    "        \n",
    "        episode_rewards_DM = 0\n",
    "        episode_rewards_Adv = 0\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "\n",
    "            # Agents decide\n",
    "            a1 = P1.act(s)\n",
    "            a2 = P2.act(s)\n",
    "            # This is to test the neutral adversary\n",
    "            #a2 = np.random.choice(2, p=[0.9, 0.1])\n",
    "\n",
    "\n",
    "            # World changes\n",
    "            s_new, rewards, done =  env.step([a1,a2])\n",
    "\n",
    "\n",
    "            # Agents learn\n",
    "\n",
    "            P1.update(s, [a1, a2], [rewards[0], rewards[1]], s_new)\n",
    "            P2.update(s, [a2, a1], [rewards[1], rewards[0]], s_new)\n",
    "            \n",
    "            s = s_new  \n",
    "            #print(r0, r1)\n",
    "            episode_rewards_DM += rewards[0]\n",
    "            episode_rewards_Adv += rewards[1]\n",
    "            \n",
    "            \n",
    "        r0s.append(episode_rewards_DM)\n",
    "        r1s.append(episode_rewards_Adv)\n",
    "        env.reset()\n",
    "        #if i%10 == 0:\n",
    "        #    P1.epsilonA *= 0.995\n",
    "        #    P1.epsilonB *= 0.995\n",
    "        #    P2.epsilon *= 0.995\n",
    "            #P1.epsilonB *= 0.9\n",
    "        \n",
    "    print(n)\n",
    "    r0ss.append(r0s)\n",
    "    r1ss.append(r1s)\n",
    "    \n",
    "plot(r0ss, r1ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "TMDP_DP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
